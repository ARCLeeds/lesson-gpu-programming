---
title: "Registers, Global, and Local Memory"
teaching: 25
exercises: 20
questions:
- "Question"
objectives:
- "Understanding the difference between registers and device memory"
- "Understanding the difference between local and global memory"
keypoints:
- "Registers can be used to store data and avoid repeated memory operations"
---

Now that we know how to write a CUDA kernel to run code on the GPU, and how to use the Python interface provided by CuPy to execute it, it is time to look at the different memory spaces in the CUDA programming model.

# Registers

Registers are fast on-chip memories that are used to store operands for the operations executed by the computing cores.

Did we encounter registers in the `vector_add` code used in the previous episode?
Yes we did! The variable `item` is, in fact, stored in a register for at least part, if not all, of a thread's execution.
In general all scalar variables defined in CUDA code are stored in registers.

Registers are local to a thread, and each thread has exclusive access to its own registers: values in registers cannot be accessed by other threads, even from the same block, and are not available for the host.
Registers are also not permanent, therefore data stored in registers is only available during the execution of a thread.

> ## Challenge: how many registers are we using?
>
> How many registers are we using in the `vector_add` code?
> 
> ~~~
> extern "C"
> __global__ void vector_add(const float * A, const float * B, float * C, const int size)
> {
>    int item = (blockIdx.x * blockDim.x) + threadIdx.x;
>    
>    if ( item < size )
>    {
>       C[item] = A[item] + B[item];
>    }
> }
> ~~~
> {: .language-c}
> > ## Solution
> > In general, it is not possible to exactly know how many registers the compiler will use without examining the output generated by the compiler itself.
> >  However, we can roughly estimate the amount of necessary registers based on the variables used.
> > We most probably need one register to store the variable `item`, two registers to store the content of `A[item]` and `B[item]`, and one additional register to store the sum `A[item] + B[item]`.
> > So the number of registers that `vector_add` probably uses is 4.
> {: .solution}
{: .challenge}

If we want to make registers use more explicit in the `vector_add` code, we can try to rewrite it in a slightly different, but equivalent, way.

~~~
extern "C"
__global__ void vector_add(const float * A, const float * B, float * C, const int size)
{
   int item = (blockIdx.x * blockDim.x) + threadIdx.x;
   float temp_a, temp_b, temp_c;

   if ( item < size )
   {
       temp_a = A[item];
       temp_b = B[item];
       temp_c = temp_a + temp_b;
       C[item] = temp_c;
   }
}
~~~
{: .language-c}

In this new version of `vector_add` we explicitly declare three `float` variables to store the values loaded from memory and the sum of our input items, making the estimation of used registers more obvious.

This it totally unnecessary in the case of our example, because the compiler will determine on its own the right amount of registers to allocate per thread, and what to store in them.
However, explicit register usage can be important for reusing items already loaded from memory.

~~~
Registers are the fastest memory on the GPU, so using them to increase data reuse is an important performance optimization.
We will look at some examples of manually using registers to improve performance in future episodes.
~~~
{: .callout}

Small CUDA arrays, which size is known at compile time, will also be allocated in registers by the compiler.
We can rewrite the previous version of `vector_add` to work with an array of registers.

~~~
extern "C"
__global__ void vector_add(const float * A, const float * B, float * C, const int size)
{
   int item = (blockIdx.x * blockDim.x) + threadIdx.x;
   float temp[3];

   if ( item < size )
   {
       temp[0] = A[item];
       temp[1] = B[item];
       temp[2] = temp[0] + temp[1];
       C[item] = temp[2];
   }
}
~~~
{: .language-c}

Once again, this is not something that we would normally do, and it is provided only as an example of how to work with arrays of registers.

# Global Memory

Accessible by the host and all threads on the GPU.
Only way to exchange data between CPU and GPU.

> ## Challenge: identify when global memory is used
>
> Observe the code of `vector_add` and identify where global memory is used.
>
> ~~~
> extern "C"
> __global__ void vector_add(const float * A, const float * B, float * C, const int size)
> {
>    int item = (blockIdx.x * blockDim.x) + threadIdx.x;
>    
>    if ( item < size )
>    {
>       C[item] = A[item] + B[item];
>    }
> }
> ~~~
> {: .language-c}
> > ## Solution
> > The vectors `A`, `B`, and `C` are in global memory.
> {: .solution}
{: .challenge}

Memory allocated on the host, and passed to the kernel as a function parameter, is allocated in global memory.
In CUDA there is no particular keyword to specify for global memory allocation.

# Local Memory

Only accessible by the thread allocating it.
All threads allocate their own local memory, but cannot see the content of the memory of the other threads.

> ## Challenge: use local memory
>
> Modify the code of `vector_add` so that intermediate data products are stored in local memory, and only the final result is saved to global memory.
>
> ~~~
> extern "C"
> __global__ void vector_add(const float * A, const float * B, float * C, const int size)
> {
>    int item = (blockIdx.x * blockDim.x) + threadIdx.x;
>    
>    if ( item < size )
>    {
>       C[item] = A[item] + B[item];
>    }
> }
> ~~~
> {: .language-c}
> Hint: have a look at the example using an array of registers.
> > ## Solution
> > 
> > We need to pass the size of the local array as a new parameter to the kernel, because if we just specified `3` in the code, the compiler would allocate registers and not local memory.
> > 
> > ~~~
> > extern "C"
> > __global__ void vector_add(const float * A, const float * B, float * C, const int size, const int local_memory_size)
> > {
> >    int item = (blockIdx.x * blockDim.x) + threadIdx.x;
> >    float local_memory[local_memory_size];
> >    
> >    if ( item < size )
> >    {
> >       local_memory[0] = A[item];
> >       local_memory[1] = B[item];
> >       local_memory[2] = local_memory[0] + local_memory[1];
> >       C[item] = local_memory[2];
> >    }
> > }
> > ~~~
> > {: .language-c}
> > 
> > The host code could be modified adding one line and changing the way the kernel is called.
> > ~~~
> > local_memory_size = 3
> > vector_add_gpu((2, 1, 1), (size // 2, 1, 1), (a_gpu, b_gpu, c_gpu, size, local_memory_size))
> > ~~~
> > {: .language-python}
> {: .solution}
{: .challenge}

It is not a fast memory, it has the same throughput and latency of global memory, but it is much larger than registers.
It is automatically used by the CUDA compiler to store spilled registers, i.e. variables that cannot be kept in registers because there is not enough space.

{% include links.md %}