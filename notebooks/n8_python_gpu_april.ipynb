{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHdJ7xI5rgKt",
        "outputId": "aa4d025d-e850-495d-adaa-168749df75c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 18 09:27:50 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# use an nvidia shell command to check the gpu\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N8 Python GPU Workshop April 2023\n",
        "\n"
      ],
      "metadata": {
        "id": "fotN0RL-sjpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our first GPU Kernel"
      ],
      "metadata": {
        "id": "hp-6x2F3s5l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vector_add that adds two input vectors together and stores the sum in another vector\n",
        "\n",
        "def vector_add(A, B, C, size):\n",
        "  for item in range(0, size):\n",
        "    C[item] = A[item] + B[item]"
      ],
      "metadata": {
        "id": "m8nytC-WsRZP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "size = 1024\n",
        "\n",
        "a_cpu = np.random.rand(size)\n",
        "b_cpu = np.random.rand(size)\n",
        "c_cpu = np.zeros(size)\n",
        "\n",
        "vector_add(a_cpu, b_cpu, c_cpu, size)"
      ],
      "metadata": {
        "id": "L9Wi-m4MvUIN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(c_cpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_OK4cp-wJZC",
        "outputId": "914b3f52-d653-4d97-d3c6-6498696768b4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.31055669 0.64728473 1.26046307 ... 0.89564628 1.30866273 1.8148962 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the above code into a CUDA kernel\n",
        "\n",
        "import cupy \n",
        "\n",
        "# CUDA kernel\n",
        "\n",
        "vector_add_cuda_code = r'''\n",
        "extern \"C\"\n",
        "__global__ void vector_add(const float * A, const float * B, float * C, const int size)\n",
        "{\n",
        "  int item = threadIdx.x;\n",
        "  C[item] = A[item] + B[item];\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "XSjwn8tGwLNu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the kernel into a function\n",
        "# we can run, through CuPy\n",
        "\n",
        "vector_add_gpu = cupy.RawKernel(vector_add_cuda_code, \"vector_add\")"
      ],
      "metadata": {
        "id": "iwMvK8XBzcB8"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the GPU version of vector_add\n",
        "\n",
        "size = 1024\n",
        "\n",
        "# create cupy arrays from numpy arrays\n",
        "a_gpu = cupy.asarray(a_cpu, dtype=cupy.float32)\n",
        "b_gpu = cupy.asarray(b_cpu, dtype=cupy.float32)\n",
        "c_gpu = cupy.zeros(size, dtype=cupy.float32)"
      ],
      "metadata": {
        "id": "r9ElkIHzzz16"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run our custom kernel on the GPU\n",
        "\n",
        "# specified 3 tuples, grid configuration, block configuration, kernel arguments\n",
        "vector_add_gpu((1, 1, 1), (size, 1, 1), (a_gpu, b_gpu, c_gpu, size))"
      ],
      "metadata": {
        "id": "or8tEsmj0HEL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if np.allclose(c_cpu, c_gpu):\n",
        "  print(\"Correct results!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eStlUsu1OV8",
        "outputId": "68f8d9f1-23c1-488d-bca4-2b81f91096aa"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct results!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using data larger than 1024\n",
        "\n",
        "size = 2048\n",
        "\n",
        "a_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "b_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "c_gpu = cupy.zeros(size, dtype=cupy.float32)"
      ],
      "metadata": {
        "id": "piBaGNJC1VkK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this errors because we've requested more than 1024 threads per block\n",
        "# in the second tuple argument\n",
        "vector_add_gpu((1,1,1), (size, 1, 1), (a_gpu, b_gpu, c_gpu, size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "HvFnjbkG6UYW",
        "outputId": "a5fb2da1-d0be-43da-dc0a-5ecbf25250a6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CUDADriverError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCUDADriverError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-8edc5c1a98fd>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this errors because we've requested more than 1024 threads per block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# in the second tuple argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvector_add_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mcupy/_core/raw.pyx\u001b[0m in \u001b[0;36mcupy._core.raw.RawKernel.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/cuda/function.pyx\u001b[0m in \u001b[0;36mcupy.cuda.function.Function.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/cuda/function.pyx\u001b[0m in \u001b[0;36mcupy.cuda.function._launch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/api/driver.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.driver.launchKernel\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/api/driver.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.driver.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mCUDADriverError\u001b[0m: CUDA_ERROR_INVALID_VALUE: invalid argument"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can we get around the block size limit\n",
        "# by request 2 blocks with half the size as number of threads\n",
        "\n",
        "vector_add_gpu((2,1,1), (size // 2, 1, 1), (a_gpu, b_gpu, c_gpu, size))"
      ],
      "metadata": {
        "id": "kap67yUw6fOW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test if this has worked as expected\n",
        "\n",
        "a_cpu = cupy.asnumpy(a_gpu)\n",
        "b_cpu = cupy.asnumpy(b_gpu)\n",
        "c_cpu = np.zeros(size, dtype=np.float32)\n",
        "\n",
        "# call the python version of vector add\n",
        "vector_add(a_cpu, b_cpu, c_cpu, size)"
      ],
      "metadata": {
        "id": "IvevRfXY7Ti1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if np.allclose(c_cpu, c_gpu):\n",
        "  print(\"Right results!\")\n",
        "else:\n",
        "  print(\"Wrong results!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyUq9Xdu7yVG",
        "outputId": "9643665e-7201-4a36-b9e6-efcc7b831368"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong results!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(c_gpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9k3qaLH8I3o",
        "outputId": "35ed3863-2068-401d-a83f-43b49b1859b9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0210285 1.1107659 1.0539565 ... 0.        0.        0.       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(c_cpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGYZouwv8LEW",
        "outputId": "121b4f26-7b1f-459f-dec8-e0cf4724c839"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0210285  1.1107659  1.0539565  ... 0.9826169  0.89512354 1.0370884 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA kernel that accounts for multiple blocks\n",
        "\n",
        "# we need to use additional CUDA variables\n",
        "# to help calculate the right index for each thread in each block\n",
        "\n",
        "\n",
        "vector_add_cuda_code = r'''\n",
        "extern \"C\"\n",
        "__global__ void vector_add(const float * A, const float * B, float * C, const int size)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  C[item] = A[item] + B[item];\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "lifct1o77651"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_add_gpu = cupy.RawKernel(vector_add_cuda_code, \"vector_add\")\n",
        "\n",
        "vector_add_gpu((2, 1, 1), (size // 2, 1, 1), (a_gpu, b_gpu, c_gpu, size))"
      ],
      "metadata": {
        "id": "YMNXnzGK9gWj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if np.allclose(c_cpu, c_gpu):\n",
        "  print(\"All correct!\")\n",
        "else:\n",
        "  print(\"Wrong results!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3jRpo5593H6",
        "outputId": "2ef0e1da-a3bc-4e0f-9154-3a5e5d7176c6"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# updating the kernel to handle arbitrary values\n",
        "\n",
        "vector_add_cuda_code = r'''\n",
        "extern \"C\"\n",
        "__global__ void vector_add(const float * A, const float * B, float * C, const int size)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  if ( item < size )\n",
        "  {\n",
        "    C[item] = A[item] + B[item];\n",
        "  }\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "DDBUYt-P9-TE"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using data of an arbitrary size\n",
        "\n",
        "size = 10_000\n",
        "\n",
        "a_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "b_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "c_gpu = cupy.zeros(size, dtype=cupy.float32)"
      ],
      "metadata": {
        "id": "Cn9hGuvv-xwn"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_add_gpu = cupy.RawKernel(vector_add_cuda_code, \"vector_add\")\n",
        "\n",
        "vector_add_gpu((2, 1, 1), (size // 2, 1, 1), (a_gpu, b_gpu, c_gpu, size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "elbnGiyq_AjC",
        "outputId": "78d89e15-a0d8-4417-fd6b-f80631451289"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CUDADriverError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCUDADriverError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-861a2ca71f88>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvector_add_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRawKernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_add_cuda_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vector_add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvector_add_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mcupy/_core/raw.pyx\u001b[0m in \u001b[0;36mcupy._core.raw.RawKernel.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/cuda/function.pyx\u001b[0m in \u001b[0;36mcupy.cuda.function.Function.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/cuda/function.pyx\u001b[0m in \u001b[0;36mcupy.cuda.function._launch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/api/driver.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.driver.launchKernel\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/api/driver.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.api.driver.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mCUDADriverError\u001b[0m: CUDA_ERROR_INVALID_VALUE: invalid argument"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the required number of blocks\n",
        "# in our CUDA grid to run data of arbitrary size\n",
        "\n",
        "import math \n",
        "\n",
        "# specify that our blocks should always have\n",
        "# 1024 threads in the x dimension\n",
        "block_size = (1024, 1, 1)\n",
        "\n",
        "grid_size = (int(math.ceil(size / 1024)), 1, 1)"
      ],
      "metadata": {
        "id": "dph6qx_n_Hmy"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_kF-lQaBJKZ",
        "outputId": "830fd2cd-2c60-4b73-f111-26eea0c01062"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can run our vector_add code on data of an arbitrary size\n",
        "# using Python logic to help calculate the CUDA grid\n",
        "vector_add_gpu(grid_size, block_size, (a_gpu, b_gpu, c_gpu, size))"
      ],
      "metadata": {
        "id": "64QRzoB_BBLQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_cpu = cupy.asnumpy(a_gpu)\n",
        "b_cpu = cupy.asnumpy(b_gpu)\n",
        "c_cpu = np.zeros(size, dtype=np.float32)\n",
        "\n",
        "vector_add(a_cpu, b_cpu, c_cpu, size)"
      ],
      "metadata": {
        "id": "ONqokRPyBOnv"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if our results match\n",
        "\n",
        "if np.allclose(c_cpu, c_gpu):\n",
        "  print(\"Correct results!\")\n",
        "else:\n",
        "  print(\"Oh no!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU7BRHidBfGQ",
        "outputId": "177a23d2-ba7c-4225-9983-6ccdad19958d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct results!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Registers, local and global memory"
      ],
      "metadata": {
        "id": "9SU5QdNQSOou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making register use more explicit\n",
        "# using temp variables to show register use\n",
        "\n",
        "vector_add_cuda_code = r'''\n",
        "extern \"C\"\n",
        "__global__ void vector_add(const float * A, const float * B, float * C, const int size)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  float temp_a, temp_b, temp_c;\n",
        "\n",
        "  if ( item < size )\n",
        "  {\n",
        "    temp_a = A[item];\n",
        "    temp_b = B[item];\n",
        "    temp_c = temp_a + temp_b;\n",
        "    C[item] = temp_c;\n",
        "  }\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "gUx5cVJOBljA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making register use more explicit\n",
        "# using temp array to show register use\n",
        "\n",
        "vector_add_cuda_code = r'''\n",
        "extern \"C\"\n",
        "__global__ void vector_add(const float * A, const float * B, float * C, const int size)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  float temp[3];\n",
        "\n",
        "  if ( item < size )\n",
        "  {\n",
        "    temp[0] = A[item];\n",
        "    temp[1] = B[item];\n",
        "    temp[2] = temp[0] + temp[1];\n",
        "    C[item] = temp[2];\n",
        "  }\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "jpYKvVgZTW6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using local memory on the thread\n",
        "\n",
        "vector_add_cuda_code = r'''\n",
        "extern \"C\"\n",
        "__global__ void vector_add(const float * A, const float * B, float * C, const int size, const int local_mem_size)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  float local_memory[local_mem_size];\n",
        "\n",
        "  if ( item < size )\n",
        "  {\n",
        "    local_memory[0] = A[item];\n",
        "    local_memory[1] = B[item];\n",
        "    local_memory[2] = local_memory[0] + local_memory[1];\n",
        "    C[item] = local_memory[2];\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "# vector_add_gpu((2,1,1), (size // 2, 1, 1), (a_gpu, b_gpu, c_gpu, size, 3)"
      ],
      "metadata": {
        "id": "kQHD0CF1VxKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking shared memory with larger than block data\n",
        "\n",
        "# using data of an arbitrary size\n",
        "\n",
        "size = 2048\n",
        "\n",
        "a_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "b_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "c_gpu = cupy.zeros(size, dtype=cupy.float32)\n",
        "\n",
        "# using shared memory\n",
        "\n",
        "vector_add_cuda_code_shared = r'''\n",
        "extern \"C\"\n",
        "__global__ void vector_add(const float * A, const float * B, float * C, const int size)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  __shared__ float temp[3];\n",
        "\n",
        "  if ( item < size )\n",
        "  {\n",
        "    temp[0] = A[item];\n",
        "    temp[1] = B[item];\n",
        "    temp[2] = temp[0] + temp[1];\n",
        "    C[item] = temp[2];\n",
        "  }\n",
        "}\n",
        "'''\n",
        "threads_per_block = 32\n",
        "grid_size = ( int(math.ceil(size / threads_per_block)), 1, 1)\n",
        "block_size = (threads_per_block, 1, 1)\n",
        "vector_add_gpu_shared = cupy.RawKernel(vector_add_cuda_code_shared, \"vector_add\")\n",
        "\n",
        "# where we're running our CUDA kernel\n",
        "vector_add_gpu_shared(\n",
        "    # specifying grid size\n",
        "    grid_size,\n",
        "    # specify block size\n",
        "    block_size,\n",
        "    # specify gpu function arguments \n",
        "    (a_gpu, b_gpu, c_gpu, size)\n",
        ")\n",
        "\n",
        "a_cpu = cupy.asnumpy(a_gpu)\n",
        "b_cpu = cupy.asnumpy(b_gpu)\n",
        "c_cpu = np.zeros(size, dtype=np.float32)\n",
        "\n",
        "vector_add(a_cpu, b_cpu, c_cpu, size)\n",
        "\n",
        "if np.allclose(c_gpu, c_gpu):\n",
        "  print(\"All good\")\n",
        "else:\n",
        "  print(\"Oh no\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZZ7PrI0fdbq",
        "outputId": "bb7134cc-f700-4c1c-f666-74b681b0f7f7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using shared memory\n",
        "\n",
        "vector_add_cuda_code_shared = r'''\n",
        "extern \"C\"\n",
        "__global__ void vector_add(const float * A, const float * B, float * C, const int size)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  int offset = threadIdx.x * 3;\n",
        "  extern __shared__ float temp[];\n",
        "\n",
        "  if ( item < size )\n",
        "  {\n",
        "    temp[offset + 0] = A[item];\n",
        "    temp[offset + 1] = B[item];\n",
        "    temp[offset + 2] = temp[offset + 0] + temp[offset + 1];\n",
        "    C[item] = temp[offset + 2];\n",
        "  }\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "K4uDcFVBXr2j"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy\n",
        "\n",
        "vector_add_gpu_shared = cupy.RawKernel(vector_add_cuda_code_shared, \"vector_add\")"
      ],
      "metadata": {
        "id": "EXWROtCHYJ77"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using data of an arbitrary size\n",
        "\n",
        "size = 2048\n",
        "\n",
        "a_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "b_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "c_gpu = cupy.zeros(size, dtype=cupy.float32)"
      ],
      "metadata": {
        "id": "-ydsucT7Ybut"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add section for handling how many blocks to request\n",
        "\n",
        "import math\n",
        "\n",
        "threads_per_block = 32\n",
        "grid_size = ( int(math.ceil(size / threads_per_block)), 1, 1)\n",
        "block_size = (threads_per_block, 1, 1)"
      ],
      "metadata": {
        "id": "i7-YGyjXb1kb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MiNT-d2cKrw",
        "outputId": "e678eade-d786-4105-c469-296d618a1d38"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# where we're running our CUDA kernel\n",
        "vector_add_gpu_shared(\n",
        "    # specifying grid size\n",
        "    grid_size,\n",
        "    # specify block size\n",
        "    block_size,\n",
        "    # specify gpu function arguments \n",
        "    (a_gpu, b_gpu, c_gpu, size), \n",
        "    # specify how much shared memory is required per block\n",
        "    shared_mem=(threads_per_block * 3 * cupy.dtype(cupy.float32).itemsize)\n",
        ")"
      ],
      "metadata": {
        "id": "1WmtL3jsYnkL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(c_gpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tZ9v9FKYwCL",
        "outputId": "6416af65-3951-4b79-e36d-dd7c301c6185"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0004283  1.1307968  0.91168463 ... 0.6923103  0.97360146 1.6778005 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a_cpu = cupy.asnumpy(a_gpu)\n",
        "b_cpu = cupy.asnumpy(b_gpu)\n",
        "c_cpu = np.zeros(size, dtype=np.float32)\n",
        "\n",
        "vector_add(a_cpu, b_cpu, c_cpu, size)"
      ],
      "metadata": {
        "id": "z_7cPei3Yx4b"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if np.allclose(c_gpu, c_gpu):\n",
        "  print(\"All good\")\n",
        "else:\n",
        "  print(\"Oh no\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd1li7TuY4Rq",
        "outputId": "65f358b0-bcd1-4db6-e6d0-4e6d2d88315a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets benchmark vector_add_gpu v vector_add_gpu_shared\n",
        "\n",
        "from cupyx.profiler import benchmark \n",
        "\n",
        "size = 2048\n",
        "\n",
        "a_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "b_gpu = cupy.random.rand(size, dtype=cupy.float32)\n",
        "c_gpu = cupy.zeros(size, dtype=cupy.float32)\n",
        "\n",
        "threads_per_block = 32\n",
        "grid_size = ( int(math.ceil(size / threads_per_block)), 1, 1)\n",
        "block_size = (threads_per_block, 1, 1)\n",
        "\n",
        "execution_gpu = benchmark(vector_add_gpu,\n",
        "                          (grid_size, block_size, (a_gpu, b_gpu, c_gpu, size)), \n",
        "                          n_repeat=10)\n",
        "\n",
        "c_gpu = cupy.zeros(size, dtype=cupy.float32)\n",
        "\n",
        "execution_gpu_shared = benchmark(vector_add_gpu_shared,\n",
        "                                 (grid_size, block_size, (a_gpu, b_gpu, c_gpu, size)),\n",
        "                                 n_repeat=10)\n",
        " \n",
        "\n",
        "print(f\"vector_add_gpu took an average of: {np.average(execution_gpu.gpu_times):.6f}\")\n",
        "\n",
        "print(f\"vector_add_gpu_shared took an average of: {np.average(execution_gpu_shared.gpu_times):.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWg7UMscY9HH",
        "outputId": "6a64a9d3-d8f8-4ea9-a448-5782b4317216"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector_add_gpu took an average of: 0.000024\n",
            "vector_add_gpu_shared took an average of: 0.000025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shared memory and a histogram function"
      ],
      "metadata": {
        "id": "gUaYlrGsjvwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram(input_array, output_array):\n",
        "  for item in input_array:\n",
        "    output_array[item] = output_array[item] + 1   "
      ],
      "metadata": {
        "id": "V8xIn47kh1_d"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_array = np.random.randint(256, size=2048, dtype=np.int32)\n",
        "\n",
        "output_array = np.zeros(256, dtype=np.int32)\n",
        "\n",
        "histogram(input_array, output_array)"
      ],
      "metadata": {
        "id": "sGuVpzgukBXx"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmS3IYTFkMrE",
        "outputId": "f7602b4e-2989-4453-ac0c-8c360154634f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 7  5  6 10  6 10 10 12  6  7  7 10  6  5 11  9  4 11  8  4 11  5  5 16\n",
            "  8 10  7 10  7 11  8 10  5  9  5  5  8  9  9  9 10 10  9  5 13  9  4  9\n",
            " 10  8 13  7  6  6  5  6  8 12  8 11  5  5  7  6  7  8  7 10  9  3 16  5\n",
            " 12  9  6  8 11 11 11  5  1  7  7  5 12  8  5 12  6  9 13  6  9  7 14  7\n",
            "  4  7  7  6 10  7 11  6  5 14  9  9  8 10  7 11  6 13  7  9  8  4  7  7\n",
            "  9  8 15  7  5 12 10  8 10  6  7 10  6 10  6  8  5 10  8  2  4  3  9 11\n",
            "  9 10 12 13 11 13  7 12  3 10  4  9  4 10  4  8 10  9  9 10 13  8  5 19\n",
            "  8 14  9  7  8  6  9  5  8 15 10  7  7  8  4  6  6 11  7  8  8  7  8 10\n",
            " 11  5  5  5 10  9  5  2  7  7  5 13  2  6  2 10  4  6 12  9 10  6  8  8\n",
            "  6  2  8 15 11  9  5  3  3  9  4  6  7 11  5  6  5 12  8 10 15  5 12 10\n",
            "  6  7  6  6  8  3 10  7  9  7  9  6  9  8  9  6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_cuda = r'''\n",
        "extern \"C\"\n",
        "__global__ void histogram(const int * input, int * output)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "\n",
        "  output[input[item]] = output[input[item]] + 1\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "MkoSvgEbkPBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [7, 5, 4, 7]\n",
        "# thread 0 - 7\n",
        "# thread 1 - 5\n",
        "# thread 2 - 4\n",
        "# thread 3 - 7\n",
        "\n",
        "# this will create a race condition where threads 0 and 3 try to update output[7]\n",
        "# at the same time overwriting one another"
      ],
      "metadata": {
        "id": "Lo8pQz0omBZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_cuda = r'''\n",
        "extern \"C\"\n",
        "__global__ void histogram(const int * input, int * output)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "\n",
        "  atomicAdd(&(output[input[item]]), 1);\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "7IFHoscXmkrB"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the GPU code versus the python code \n",
        "\n",
        "size = 2 ** 25\n",
        "\n",
        "# create some input data for cpu and gpu\n",
        "\n",
        "input_gpu = cupy.random.randint(256, size=size, dtype=cupy.int32)\n",
        "input_cpu = cupy.asnumpy(input_gpu)\n",
        "output_gpu = cupy.zeros(256, dtype=cupy.int32)\n",
        "output_cpu = cupy.asnumpy(output_gpu)\n",
        "\n",
        "# compile and setup CUDA kernel\n",
        "\n",
        "histogram_gpu = cupy.RawKernel(histogram_cuda, \"histogram\")\n",
        "threads_per_block = 256 \n",
        "grid_size = (int(math.ceil(size / threads_per_block)), 1, 1)\n",
        "block_size = (threads_per_block, 1, 1)\n",
        "\n",
        "# correctness check\n",
        "\n",
        "histogram(input_cpu, output_cpu)\n",
        "histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))\n",
        "if np.allclose(output_cpu, output_gpu):\n",
        "  print(\"Correct results!\")\n",
        "else:\n",
        "  print(\"Oh no!\")\n",
        "\n",
        "# measure performance\n",
        "\n",
        "%timeit -n 1 -r 1 histogram(input_cpu, output_cpu)\n",
        "execution_gpu = benchmark(histogram_gpu, \n",
        "                          (grid_size, block_size, (input_gpu, output_gpu)), \n",
        "                           n_repeat=10)\n",
        "gpu_avg_time = np.average(execution_gpu.gpu_times)\n",
        "print(f\"{gpu_avg_time:.6f} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlWCVtuKnCqv",
        "outputId": "d65309fa-d41d-4d3c-e80a-8047f305d783"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct results!\n",
            "1min 25s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "0.013137 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using shared memory to prevent conflicts in global memory\n",
        "\n",
        "histogram_cuda_shared = r'''\n",
        "extern \"C\"\n",
        "__global__ void histogram(const int * input, int * output)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  __shared__ int temp_histogram[256];\n",
        "\n",
        "  // initialise shared memory\n",
        "  temp_histogram[threadIdx.x] = 0;\n",
        "  __syncthreads();\n",
        "\n",
        "  // update block shared memory for value in item\n",
        "  atomicAdd(&(temp_histogram[input[item]]), 1);\n",
        "  __syncthreads();\n",
        "\n",
        "  // update the global histogram (output) using temporary histogram\n",
        "  atomicAdd(&(output[threadIdx.x]), temp_histogram[threadIdx.x]);\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "jB_FhUEPogZN"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the GPU code using shared memory versus the python code \n",
        "\n",
        "size = 2 ** 25\n",
        "\n",
        "# create some input data for cpu and gpu\n",
        "\n",
        "input_gpu = cupy.random.randint(256, size=size, dtype=cupy.int32)\n",
        "input_cpu = cupy.asnumpy(input_gpu)\n",
        "output_gpu = cupy.zeros(256, dtype=cupy.int32)\n",
        "output_cpu = cupy.asnumpy(output_gpu)\n",
        "\n",
        "# compile and setup CUDA kernel\n",
        "\n",
        "histogram_gpu_shared = cupy.RawKernel(histogram_cuda_shared, \"histogram\")\n",
        "threads_per_block = 256 \n",
        "grid_size = (int(math.ceil(size / threads_per_block)), 1, 1)\n",
        "block_size = (threads_per_block, 1, 1)\n",
        "\n",
        "# correctness check\n",
        "\n",
        "histogram(input_cpu, output_cpu)\n",
        "histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))\n",
        "histogram_gpu_shared(grid_size, block_size, (input_gpu, output_gpu))\n",
        "# worth noting that output_gpu is generated from _shared not _gpu \n",
        "if np.allclose(output_cpu, output_gpu):\n",
        "  print(\"Correct results!\")\n",
        "else:\n",
        "  print(\"Oh no!\")\n",
        "\n",
        "# measure performance\n",
        "\n",
        "%timeit -n 1 -r 1 histogram(input_cpu, output_cpu)\n",
        "execution_gpu = benchmark(histogram_gpu, \n",
        "                          (grid_size, block_size, (input_gpu, output_gpu)), \n",
        "                           n_repeat=10)\n",
        "execution_gpu_shared = benchmark(histogram_gpu_shared, \n",
        "                          (grid_size, block_size, (input_gpu, output_gpu)), \n",
        "                           n_repeat=10)\n",
        "gpu_avg_time = np.average(execution_gpu.gpu_times)\n",
        "gpu_avg_time_shared = np.average(execution_gpu_shared.gpu_times)\n",
        "print(f\"{gpu_avg_time:.6f} s\")\n",
        "print(f\"{gpu_avg_time_shared:.6f} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2csIh4_YxT6U",
        "outputId": "0d8091c8-5122-4cd1-87b1-c05112a2d3aa"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh no!\n",
            "1min 39s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "0.014929 s\n",
            "0.001898 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constant data on GPU"
      ],
      "metadata": {
        "id": "nWK3ilp61C4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can define constant data that is accessible to all threads but is read-only\n",
        "\n",
        "constant_kernel = r'''\n",
        "extern \"C\" {\n",
        "#define BLOCKS 2 \n",
        "\n",
        "__constant__ float factors[BLOCKS];\n",
        "\n",
        "__global__ void sum_and_multiply(const float * A, const float * B, float * C, const int size)\n",
        "{\n",
        "  int item = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "  C[item] = (A[item] + B[item]) * factors[blockIdx.x];\n",
        "}\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "fefqB-JByFhq"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module = cupy.RawModule(code = constant_kernel)"
      ],
      "metadata": {
        "id": "g5dBh8JO0MGm"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a pointer to the memory location of the constant\n",
        "# variable factors\n",
        "factor_ptr = module.get_global(\"factors\")"
      ],
      "metadata": {
        "id": "S5qH441a0UwE"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create some data on the host\n",
        "factors_gpu = cupy.ndarray(2, cupy.float32, factor_ptr)"
      ],
      "metadata": {
        "id": "LcyFyTdZ0eBn"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factors_gpu[...] = cupy.random.random(2, dtype=cupy.float32)"
      ],
      "metadata": {
        "id": "zjcKLeVG0xZx"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concurrency on GPU"
      ],
      "metadata": {
        "id": "4QwgRr3K1Fdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_add_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlJnsuv506EG",
        "outputId": "a61fe22c-2e89-4ea8-8bf1-9bff19172acb"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cupy._core.raw.RawKernel at 0x7fa5d3441f40>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stream_one = cupy.cuda.Stream() \n",
        "stream_two = cupy.cuda.Stream()"
      ],
      "metadata": {
        "id": "eSKJrf4Q1bG0"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use streams to run two kernels without one depending on the other\n",
        "\n",
        "c_gpu = cupy.zeros(size, dtype=cupy.float32)\n",
        "\n",
        "with stream_one:\n",
        "  histogram_gpu_shared(grid_size, block_size, (input_gpu, output_gpu))\n",
        "\n",
        "with stream_two: \n",
        "  histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))"
      ],
      "metadata": {
        "id": "8L1l8ou02BlR"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using cuda events to manage concurrent kernel runs\n",
        "\n",
        "stream_one = cupy.cuda.Stream()\n",
        "stream_two = cupy.cuda.Stream()\n",
        "sync_point = cupy.cuda.Event()\n",
        "\n",
        "with stream_one:\n",
        "  histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))\n",
        "  sync_point.record(stream=stream_one)\n",
        "  histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))\n",
        "\n",
        "with stream_two:\n",
        "  stream_two.wait_event(sync_point)\n",
        "  histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))\n"
      ],
      "metadata": {
        "id": "JJ01N3dj22bk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}